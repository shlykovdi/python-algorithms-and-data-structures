Извините, я снова использовал готовый велосипед "from memory_profiler import profile".

Результаты замеров памяти проводились отдельно для каждой функции в отдельном файле
отдельным запуском интерпретатора.

Результаты замеров:
# Result-1: 36.83594 MiB - 18.42188 MiB = 19308541,379 bytes (~19,30 Mb)
# Result-2: 36.79688 MiB - 18.27344 MiB = 19423234,621 bytes (~19,42 Mb)
# Result-3: 36.73828 MiB - 18.33594 MiB = 19296252,068 bytes (~19,29 Mb)

По результатам замеров можно сказать, что 3 алгоритм занял меньше всего памяти. 
Объясняется это тем, что не использовались zip() и enumerate(), которые создавали бы новые кортежи.
Обход массива по индексам оказался более экономичным по памяти, но не факт,
что более быстрым или более питонячим.
В конце была использована системная функция max(), что также сократило код и потребление памяти.

***********************************
Если говорить о проявлении "творчества и фантазии", то можно посмотреть статью на этом сайте:
https://code.tutsplus.com/ru/tutorials/understand-how-much-memory-your-python-objects-use--cms-25609

Функция "deep_getsizeof(...)" должна корректно замерять размеры объектов, но не понятно 
как именно ее применять, чтобы получить замеры, например в тех же лист компрехеншенах.
Внутри if, for, while, где есть локальные временные переменные. Вероятным решением видится
опрос переменных через locals(), но как именно и в какой момент опрашивать? Получатся точечные
замеры и то они не дадут достоверного знания о том, сколько действительно было съедено памяти.

Поэтому я решил использовать готовый профилировщик, который, вероятно, учитывает все тонкости,
замеряет действительно используемую память и т.д.
